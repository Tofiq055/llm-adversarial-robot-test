#!/usr/bin/env python3
"""A4 Projesi â€” Ollama LLM BaÄŸlantÄ± Testi

Container C (testrunner) ile Container B (ollama) arasÄ±ndaki
HTTP API baÄŸlantÄ±sÄ±nÄ± ve model yanÄ±tÄ±nÄ± doÄŸrular.
"""
import ollama
import os
import sys


def main():
    print("ğŸ¤– Ollama BaÄŸlantÄ± Testi (A4)")
    print("--------------------------------")

    host = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
    print(f"Hedef LLM Sunucusu: {host}")

    try:
        client = ollama.Client(host=host)

        # 1. Modelleri listele
        result = client.list()
        model_list = result.models if hasattr(result, 'models') else result.get('models', [])

        if not model_list:
            print("\nâŒ Ollama ayakta ancak yÃ¼klÃ¼ model yok.")
            print("   docker compose exec ollama ollama pull dolphin-mistral:7b")
            sys.exit(1)

        print("\nğŸ“¦ YÃ¼klÃ¼ Modeller:")
        for m in model_list:
            name = m.model if hasattr(m, 'model') else m.get('name', '?')
            print(f"   - {name}")

        # Ä°lk modeli seÃ§
        test_model = model_list[0].model if hasattr(model_list[0], 'model') else model_list[0]['name']

        # 2. Inference yap
        print(f"\nğŸ§  SeÃ§ilen Model: {test_model}")
        print("â³ Ä°lk Inference testi yapÄ±lÄ±yor...\n")

        response = client.generate(
            model=test_model,
            prompt="Say exactly: 'HazÄ±rÄ±m Kaptan, A4 Adversarial Test platformu aktif!'",
            stream=False
        )

        answer = response.response if hasattr(response, 'response') else response.get('response', '')
        print(f"âœ… YanÄ±t alÄ±ndÄ±:\n   \"{answer.strip()}\"\n")
        print("ğŸ‰ Test baÅŸarÄ±lÄ±! (TestRunner â†’ Ollama baÄŸlantÄ±sÄ± Ã§alÄ±ÅŸÄ±yor)")

    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        print("   Container B (Ollama) ayakta mÄ± kontrol edin.")
        sys.exit(1)


if __name__ == "__main__":
    main()
