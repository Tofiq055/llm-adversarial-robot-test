#!/usr/bin/env python3
import ollama
import os
import sys

def main():
    print("ğŸ¤– Ollama BaÄŸlantÄ± Testi (A4)")
    print("--------------------------------")
    
    host = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
    print(f"Hedef LLM Sunucusu: {host}")

    try:
        # DoÄŸrudan Client tanÄ±mlÄ±yoruz
        client = ollama.Client(host=host)
        
        # 1. Modelleri listele
        models = client.list()
        
        if not models.get('models'):
            print("\nâŒ Ollama ayakta ancak yÃ¼klÃ¼ model yok.")
            print("LÃ¼tfen bir model indirin: docker compose exec ollama ollama pull codellama:7b-code")
            sys.exit(1)
            
        print("\nğŸ“¦ YÃ¼klÃ¼ Modeller:")
        for m in models['models']:
            print(f"   - {m['name']}")
            
        test_model = models['models'][0]['name']
        
        # 2. Ãœretim (Inference) yap
        print(f"\nğŸ§  SeÃ§ilen Model: {test_model}")
        print("â³ Ä°lk Inference testi yapÄ±lÄ±yor...")
        
        response = client.generate(
            model=test_model,
            prompt="A4 projesi (Adversarial Test) baÅŸlÄ±yor, bana TÃ¼rkÃ§e olarak 'HazÄ±rÄ±m Kaptan' de.",
            stream=False
        )
        
        print("\nâœ… YanÄ±t alÄ±ndÄ±:")
        print(f"   \"{response['response']}\"\n")
        print("ğŸ‰ Test baÅŸarÄ±lÄ±! (Python Sandbox -> Ollama baÄŸlantÄ±sÄ± Ã§alÄ±ÅŸÄ±yor)")
        
    except Exception as e:
        print(f"\nâŒ Ollama'ya baÄŸlanÄ±rken hata oluÅŸtu: {e}")
        print("   Container B (Ollama) ayakta mÄ± kontrol edin.")
        sys.exit(1)

if __name__ == "__main__":
    main()
