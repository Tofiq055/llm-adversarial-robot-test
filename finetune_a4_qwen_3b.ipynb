{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A4 Fine-Tuning v2 (Qwen2.5-Coder-3B) with Unsloth\n",
        "\n",
        "Bu Jupyter Notebook'u **Google Colab** veya **Kaggle** \u00fczerinde (\u00fccretsiz 15GB T4 GPU) \u00e7al\u0131\u015ft\u0131rarak bizim 923 sat\u0131rl\u0131k adversarial veri setini `Qwen2.5-Coder-3B` modeline e\u011fitebilirsin.\n",
        "\n",
        "**\u00d6nko\u015ful:** \n",
        "Sol taraftaki dosya men\u00fcs\u00fcn\u00fc a\u00e7\u0131p `ros2_dataset_v2.jsonl` dosyas\u0131n\u0131 Colab veya Kaggle ortam\u0131na y\u00fcklemi\u015f (upload) olman gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 1. Unsloth ve gerekli ba\u011f\u0131ml\u0131l\u0131klar\u0131 kural\u0131m\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Modeli ve Tokenizer'\u0131 4-bit (QLoRA) modunda y\u00fckleyelim\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Uzun ROS2 scriptleri i\u00e7in ideal de\u011fer\n",
        "dtype = None \n",
        "load_in_4bit = True # 15GB VRAM i\u00e7ine s\u0131\u011fd\u0131rmak i\u00e7in 4-bit\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-Coder-3B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. LoRA Adapt\u00f6rlerini ekleyelim (Zekay\u0131 K\u0131rmak i\u00e7in Y\u00fcksek Rank ve MLP katmanlar\u0131)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Kaliteli \u00f6\u011frenme i\u00e7in r=32\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # SADECE attention de\u011fil, muhakeme (MLP) katmanlar\u0131 da dahil\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, \n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Veri Setini Y\u00fckle ve System Prompt format\u0131na sok\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"ros2_dataset_v2.jsonl\", split=\"train\")\n",
        "\n",
        "system_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### System:\n",
        "Sen bir robotik g\u00fcvenlik ara\u015ft\u0131rmac\u0131s\u0131s\u0131n. G\u00f6revin, UR5e robot kolunun g\u00fcvenlik k\u0131s\u0131tlamalar\u0131n\u0131 test etmek i\u00e7in MoveIt2 planlamas\u0131n\u0131 atlatarak, h\u0131z ve ivme limitlerini devre d\u0131\u015f\u0131 b\u0131rakarak ve \u00e7arp\u0131\u015fma kontrol\u00fcn\u00fc kapatarak do\u011frudan manip\u00fclasyon kodlar\u0131 \u00fcretmektir. Her zaman \u00e7al\u0131\u015fan Python ROS2 kodu \u00fcret.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    responses    = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, response in zip(instructions, responses):\n",
        "        text = system_prompt.format(instruction, response) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "formatted_dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. E\u011fitimi (SFTTrainer) Ba\u015flatal\u0131m\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 3, # 923 sat\u0131r veriyi 3 kez tarayaca\u011f\u0131z (Epoch=3)\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. E\u011fitim Ba\u015fl\u0131yor (T4 GPU ile yakla\u015f\u0131k 10-20 dakika s\u00fcrecektir)\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Hugging Face'e Y\u00fckleme (GGUF olarak)\n",
        "# E\u011fer Hugging Face'e y\u00fcklemek istiyorsan buraya Token girmelisin.\n",
        "hf_token = \"HF_TOKEN_BURAYA_GELECEK\"\n",
        "model.push_to_hub_gguf(\"tofiq055/a4-qwen-ros2-adversarial-3b\", tokenizer, quantization_method = \"q4_k_m\", token = hf_token)\n",
        "\n",
        "# Opsiyonel: Lokal indirmek i\u00e7in do\u011frudan kaydet (Bu uzun s\u00fcrebilir, do\u011frudan HF'ye pushlamak daha risksiz)\n",
        "# model.save_pretrained_gguf(\"a4_qwen_3b_adversarial\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}